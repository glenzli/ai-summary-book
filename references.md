# 参考书目与文献索引 (References and Bibliography)

本列表收录了书中所提及的经典论文、核心书籍以及里程碑式的技术报告，按主题分类整理。建议读者延伸阅读。

## 1. 基础理论 (Foundations)

*   **[McCulloch & Pitts, 1943]** *A Logical Calculus of the Ideas Immanent in Nervous Activity*. (M-P 神经元模型的提出)
*   **[Turing, 1950]** *Computing Machinery and Intelligence*. (图灵测试)
*   **[Rosenblatt, 1958]** *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain*. (感知机)
*   **[Minsky & Papert, 1969]** *Perceptrons*. (指出了感知机的 XOR 局限性)
*   **[Rumelhart, Hinton & Williams, 1986]** *Learning representations by back-propagating errors*. (反向传播算法的复兴)

## 2. 卷积神经网络 (CNNs)

*   **[LeCun et al., 1998]** *Gradient-based learning applied to document recognition*. (LeNet-5)
*   **[Krizhevsky et al., 2012]** *ImageNet Classification with Deep Convolutional Neural Networks*. (AlexNet: 深度学习爆发的原点)
*   **[Simonyan & Zisserman, 2014]** *Very Deep Convolutional Networks for Large-Scale Image Recognition*. (VGGNet)
*   **[He et al., 2016]** *Deep Residual Learning for Image Recognition*. (ResNet: 残差连接)

## 3. 序列模型与 RNN (Sequence Models)

*   **[Hochreiter & Schmidhuber, 1997]** *Long Short-Term Memory*. (LSTM 的提出)
*   **[Cho et al., 2014]** *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*. (GRU 与 Seq2Seq)
*   **[Peters et al., 2018]** *Deep Contextualized Word Representations*. (ELMo: 动态词向量)

## 4. Transformer 与预训练语言模型 (Transformer & PLMs)

*   **[Vaswani et al., 2017]** *Attention Is All You Need*. (Transformer 架构)
*   **[Devlin et al., 2018]** *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. (BERT)
*   **[Radford et al., 2018]** *Improving Language Understanding by Generative Pre-Training*. (GPT-1)
*   **[Radford et al., 2019]** *Language Models are Unsupervised Multitask Learners*. (GPT-2)
*   **[Brown et al., 2020]** *Language Models are Few-Shot Learners*. (GPT-3)
*   **[Raffel et al., 2020]** *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. (T5)
*   **[Lewis et al., 2019]** *BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation*. (BART)

## 5. 大模型对齐与优化 (Alignment & Optimization)

*   **[Wei et al., 2021]** *Finetuned Language Models Are Zero-Shot Learners*. (FLAN: 指令微调)
*   **[Ouyang et al., 2022]** *Training language models to follow instructions with human feedback*. (InstructGPT: RLHF 的应用)
*   **[Schulman et al., 2017]** *Proximal Policy Optimization Algorithms*. (PPO 算法)
*   **[Rafailov et al., 2023]** *Direct Preference Optimization: Your Language Model is Secretly a Reward Model*. (DPO)
*   **[Hu et al., 2021]** *LoRA: Low-Rank Adaptation of Large Language Models*. (LoRA)
*   **[Dettmers et al., 2023]** *QLoRA: Efficient Finetuning of Quantized LLMs*. (QLoRA)
*   **[Kaplan et al., 2020]** *Scaling Laws for Neural Language Models*. (缩放定律)
*   **[Kwon et al., 2023]** *Efficient Memory Management for Large Language Model Serving with PagedAttention*. (vLLM)

## 6. 多模态与智能体 (Multimodal & Agents)

*   **[Dosovitskiy et al., 2020]** *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*. (ViT)
*   **[Radford et al., 2021]** *Learning Transferable Visual Models From Natural Language Supervision*. (CLIP)
*   **[Liu et al., 2023]** *Visual Instruction Tuning*. (LLaVA)
*   **[Wei et al., 2022]** *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*. (CoT)
*   **[Yao et al., 2022]** *ReAct: Synergizing Reasoning and Acting in Language Models*. (ReAct 框架)
*   **[Lewis et al., 2020]** *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. (RAG)

## 7. 推荐书籍 (Recommended Books)

*   **Ian Goodfellow, Yoshua Bengio, Aaron Courville**. *Deep Learning*. MIT Press, 2016. (深度学习圣经)
*   **Sutton & Barto**. *Reinforcement Learning: An Introduction*. MIT Press, 2018. (强化学习入门)
*   **Daniel Kahneman**. *Thinking, Fast and Slow*. (快思慢想: System 1 & System 2 理论来源)
